{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "e9wGadcSx8hy"
      },
      "source": [
        "# Mission 1-1\n",
        "Q. 주어진 이미지 데이터의 파일 명에 기반하여 \"이미지 ID\" 수 기준으로 \"성별 & 스타일\" 통계치를 낸다.\n",
        "\n",
        "\n",
        "통계를 내기에 앞서 Training 데이터에 중복된 \"이미지 ID\"가 존재하므로 전처리하여 제거하는 과정을 거쳤다.\n",
        "(제거된 이미지 ID: 02958, 09717, 10028, 17802)\n",
        "\n",
        "Validation 데이터 내에서는 중복된 데이터가 존재하지 않았다.\n",
        "\n",
        "1. 폴더에서 이미지 파일 처리:\n",
        "os.listdir(folder_path)로 지정된 폴더 내 모든 파일을 순회하며, 확장자가 .jpg인 파일만 처리합니다.\n",
        "extract_info_from_filename을 호출하여 각 이미지 파일에서 추출된 정보를 data 리스트에 저장합니다.\n",
        "\n",
        "2. DataFrame으로 변환:\n",
        "data 리스트를 pd.DataFrame(data)로 변환하여 데이터프레임 df에 저장합니다. 이 데이터프레임에는 W_T, image_id, era, style, gender, filename 열이 포함됩니다.\n",
        "\n",
        "3. 중복된 이미지 ID 확인 및 제거:\n",
        "- df[df.duplicated(subset='image_id', keep=False)]로 중복된 image_id를 가진 항목을 필터링하여 duplicates 데이터프레임에 저장하고, 중복된 파일들의 목록을 출력합니다.\n",
        "- 마지막에 df[df.duplicated(subset='image_id', keep=False) == False]로 중복되지 않은 데이터만을 필터링하여 반환합니다.\n",
        "\n",
        "Mission 1 요구 사항:\n",
        "중복된 image_id 확인 및 제거 작업은 중복 데이터를 처리해야 한다는 요구 사항을 만족합니다.\n",
        "\n",
        "4. 학습 및 검증 데이터 폴더 설정:\n",
        "train_folder = 'data/training_image'\n",
        "valid_folder = 'data/validation_image'\n",
        "- 이 부분은 학습 및 검증 이미지 데이터가 저장된 폴더 경로를 설정합니다. 이 경로를 기준으로 process_folder 함수에서 파일을 불러옵니다.\n",
        "\n",
        "5. 학습 및 검증 데이터 처리:\n",
        "train_df = process_folder(train_folder)\n",
        "valid_df = process_folder(valid_folder)\n",
        "- process_folder 함수를 호출하여 학습 및 검증 데이터 폴더의 이미지를 각각 train_df와 valid_df 데이터프레임으로 처리합니다. 이 데이터프레임에는 이미지 파일명에서 추출한 성별 및 스타일 정보가 포함됩니다.\n",
        "\n",
        "6. 성별 및 스타일별 이미지 개수 세기:\n",
        "train_counts = train_df.groupby(['gender', 'style']).size().reset_index(name='count')\n",
        "valid_counts = valid_df.groupby(['gender', 'style']).size().reset_index(name='count')\n",
        "- train_df와 valid_df에서 groupby(['gender', 'style'])로 성별 및 스타일 조합별로 그룹화하여 각 그룹에 속하는 이미지 개수를 셉니다.\n",
        "size()는 각 그룹의 크기를 계산하며, reset_index(name='count')로 결과를 DataFrame 형태로 정리합니다.\n",
        "\n",
        "Mission 1 요구 사항:\n",
        "train_counts와 valid_counts는 성별 및 스타일별 이미지 수를 나타내는 통계표를 작성해야 한다는 요구 사항을 충족합니다.\n",
        "\n",
        "7. 결과 출력:\n",
        "print(\"\\nTrain Image Counts by Gender and Style:\")\n",
        "print(train_counts)\n",
        "print(\"\\nValidation Image Counts by Gender and Style:\")\n",
        "print(valid_counts)\n",
        "- 위에서 구한 train_counts와 valid_counts를 출력하여 학습 및 검증 데이터의 성별 및 스타일별 이미지 수 통계를 확인할 수 있습니다.\n",
        "\n",
        "1. 클래스 조합 추출 및 정렬\n",
        "- # 추출된 클래스 조합 확인 및 정렬\n",
        "class_combinations = train_df[['gender', 'style']].drop_duplicates()\n",
        "class_combinations = class_combinations.sort_values(['gender', 'style'], ascending=[False, True])\n",
        "- class_combinations: 학습 데이터셋(train_df)에서 gender와 style의 조합을 중복 없이 추출합니다.\n",
        "- 정렬: gender를 기준으로 내림차순, style을 기준으로 오름차순 정렬하여 순서를 일관되게 만듭니다. 이는 이후 클래스 ID 할당 시 순서를 명확히 하기 위함입니다.\n",
        "\n",
        "Mission 1 요구 사항:\n",
        "데이터에서 고유한 성별과 스타일 조합을 확인하고, 이를 통해 학습에 필요한 클래스 정보를 준비합니다.\n",
        "\n",
        "2. 클래스 매핑 생성\n",
        "- # 정렬된 클래스 조합을 기반으로 클래스 매핑 생성\n",
        "class_mapping = {tuple(row): idx for idx, row in class_combinations.reset_index(drop=True).iterrows()}\n",
        "print(\"\\nClass mapping:\")\n",
        "print(class_mapping)\n",
        "- class_mapping: 정렬된 class_combinations를 반복하며 gender, style 조합을 키로 하고 해당 순서 인덱스(idx)를 값으로 하는 딕셔너리를 생성합니다.\n",
        "- 예를 들어, ('남성', 'hippie')가 0번 클래스라면, class_mapping에 {('남성', 'hippie'): 0}이 저장됩니다.\n",
        "- 클래스 매핑 출력: class_mapping을 출력하여 각 조합의 클래스 ID를 확인할 수 있습니다.\n",
        "\n",
        "Mission 1 요구 사항:\n",
        "성별 및 스타일 조합을 고유한 클래스 ID로 변환하여 모델 학습 시 사용할 수 있도록 준비합니다.\n",
        "\n",
        "3. 클래스 ID 할당\n",
        "- # 클래스 ID 할당\n",
        "train_df['class_id'] = train_df.apply(lambda row: class_mapping[(row['gender'], row['style'])], axis=1)\n",
        "valid_df['class_id'] = valid_df.apply(lambda row: class_mapping[(row['gender'], row['style'])], axis=1)\n",
        "- 학습(train_df)과 검증(valid_df) 데이터프레임에 class_id 열을 추가하여, 각 이미지에 대해 해당 성별 및 스타일 조합의 클래스 ID를 할당합니다.\n",
        "- apply와 lambda를 사용하여 각 행의 gender와 style 조합을 class_mapping 딕셔너리에서 찾고, 이에 해당하는 클래스 ID를 할당합니다.\n",
        "\n",
        "Mission 1 요구 사항:\n",
        "- 각 이미지에 대해 고유한 클래스 ID를 부여하여, 학습 및 검증 데이터셋을 모델에 입력하기에 적합한 형태로 준비합니다.\n",
        "\n",
        "4. 클래스 매핑 테이블 생성 및 출력\n",
        "- # 클래스 매핑 테이블 생성\n",
        "class_mapping_table = pd.DataFrame(list(class_mapping.keys()), columns=['Gender', 'Style'])\n",
        "class_mapping_table['Class ID'] = list(class_mapping.values())\n",
        "class_mapping_table = class_mapping_table[['Class ID', 'Gender', 'Style']]\n",
        "print(\"\\nClass Mapping Table:\")\n",
        "print(class_mapping_table)\n",
        "- class_mapping 딕셔너리를 DataFrame으로 변환하여 Class ID, Gender, Style 열을 가지는 매핑 테이블을 만듭니다. 이 테이블은 각 클래스 ID가 어떤 성별 및 - 스타일 조합에 해당하는지 보여줍니다.\n",
        "출력하여 각 클래스 ID와 그에 해당하는 성별, 스타일 조합을 확인할 수 있습니다.\n",
        "\n",
        "Mission 1 요구 사항:\n",
        "각 클래스 ID에 해당하는 성별과 스타일 조합을 명확히 정의하여 모델 해석 및 분석에 용이하도록 만듭니다.\n",
        "\n",
        "5. 데이터 저장\n",
        "- # CSV 파일로 저장\n",
        "train_df.to_csv('data/train_metadata_with_class.csv', index=False)\n",
        "valid_df.to_csv('data/valid_metadata_with_class.csv', index=False)\n",
        "- train_df와 valid_df를 CSV 파일로 저장하여, 학습 및 검증 데이터셋에 대한 메타데이터 및 클래스 ID 정보를 보존합니다.\n",
        "- train_metadata_with_class.csv 및 valid_metadata_with_class.csv 파일은 이후 분석 및 모델 학습 시 유용하게 활용할 수 있습니다.\n",
        "\n",
        "6. GPU 사용 가능 여부 확인\n",
        "import torch\n",
        "# GPU 사용 가능 여부 확인\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# 그래픽카드 정보 출력\n",
        "if torch.cuda.is_available():\n",
        "    print(\"\\nGPU:\")\n",
        "    print(torch.cuda.get_device_name(device))\n",
        "else:\n",
        "    print(\"\\nGPU: Not available\")\n",
        "- torch.cuda.is_available()로 GPU가 사용 가능한지 확인하고, 사용 가능한 경우 torch.device('cuda')로 GPU 장치를 설정합니다.\n",
        "- GPU가 사용 가능한 경우 그래픽카드 이름을 출력하고, 사용 불가능한 경우 \"GPU: Not available\"을 출력합니다.\n",
        "\n",
        "Mission 1 요구 사항:\n",
        "학습 속도 최적화를 위해 GPU 사용 가능 여부를 확인하고, 가능하다면 GPU 장치에 모델을 할당할 준비를 합니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-30T06:39:50.780897Z",
          "start_time": "2024-10-30T06:39:49.771650Z"
        },
        "id": "8iHShlxNx8h1",
        "outputId": "da9bd51b-bb76-4d05-b84e-f56db3cfe386"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "중복된 image_id 목록 (data/training_image):\n",
            "image_id                        filename\n",
            "   02958       T_02958_19_normcore_M.jpg\n",
            "   02958           W_02958_60_mods_M.jpg\n",
            "   09717     T_09717_19_genderless_W.jpg\n",
            "   09717       W_09717_19_normcore_W.jpg\n",
            "   10028        W_10028_50_classic_W.jpg\n",
            "   10028 T_10028_10_sportivecasual_W.jpg\n",
            "   17802       T_17802_19_normcore_M.jpg\n",
            "   17802           W_17802_80_bold_M.jpg\n",
            "\n",
            "중복된 image_id 목록 (data/validation_image):\n",
            "Empty DataFrame\n",
            "Columns: [image_id, filename]\n",
            "Index: []\n",
            "\n",
            "Train Image Counts by Gender and Style:\n",
            "   gender           style  count\n",
            "0       M            bold    267\n",
            "1       M          hiphop    274\n",
            "2       M          hippie    260\n",
            "3       M             ivy    237\n",
            "4       M     metrosexual    278\n",
            "5       M            mods    268\n",
            "6       M        normcore    362\n",
            "7       M  sportivecasual    298\n",
            "8       W      athleisure     67\n",
            "9       W   bodyconscious     95\n",
            "10      W        cityglam     67\n",
            "11      W         classic     76\n",
            "12      W           disco     37\n",
            "13      W         ecology     64\n",
            "14      W        feminine    154\n",
            "15      W      genderless     76\n",
            "16      W          grunge     31\n",
            "17      W          hiphop     48\n",
            "18      W          hippie     91\n",
            "19      W          kitsch     91\n",
            "20      W        lingerie     55\n",
            "21      W          lounge     45\n",
            "22      W        military     33\n",
            "23      W         minimal    139\n",
            "24      W        normcore    152\n",
            "25      W        oriental     78\n",
            "26      W          popart     41\n",
            "27      W       powersuit    120\n",
            "28      W            punk     65\n",
            "29      W           space     37\n",
            "30      W  sportivecasual    156\n",
            "\n",
            "Validation Image Counts by Gender and Style:\n",
            "   gender           style  count\n",
            "0       M            bold     57\n",
            "1       M          hiphop     66\n",
            "2       M          hippie     82\n",
            "3       M             ivy     79\n",
            "4       M     metrosexual     58\n",
            "5       M            mods     80\n",
            "6       M        normcore     51\n",
            "7       M  sportivecasual     52\n",
            "8       W      athleisure     14\n",
            "9       W   bodyconscious     23\n",
            "10      W        cityglam     18\n",
            "11      W         classic     22\n",
            "12      W           disco     10\n",
            "13      W         ecology     17\n",
            "14      W        feminine     44\n",
            "15      W      genderless     12\n",
            "16      W          grunge     10\n",
            "17      W          hiphop      8\n",
            "18      W          hippie     14\n",
            "19      W          kitsch     22\n",
            "20      W        lingerie      5\n",
            "21      W          lounge      8\n",
            "22      W        military      9\n",
            "23      W         minimal     35\n",
            "24      W        normcore     20\n",
            "25      W        oriental     18\n",
            "26      W          popart      8\n",
            "27      W       powersuit     34\n",
            "28      W            punk     12\n",
            "29      W           space     15\n",
            "30      W  sportivecasual     48\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 파일명에서 정보 추출하는 함수\n",
        "def extract_info_from_filename(filename):\n",
        "    parts = filename.split('_')\n",
        "    # gender = '여성' if parts[4].split('.')[0] == 'W' else '남성'\n",
        "    return {\n",
        "        'W_T': parts[0],\n",
        "        'image_id': parts[1],\n",
        "        'era': parts[2],\n",
        "        'style': parts[3],\n",
        "        'gender': parts[4].split('.')[0],\n",
        "        'filename': filename\n",
        "    }\n",
        "\n",
        "# 폴더 내 이미지 파일 처리하는 함수\n",
        "def process_folder(folder_path):\n",
        "    data = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.jpg'):\n",
        "            info = extract_info_from_filename(filename)\n",
        "            data.append(info)\n",
        "    # DataFrame으로 변환\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # 중복된 image_id를 가진 항목들 확인\n",
        "    duplicates = df[df.duplicated(subset='image_id', keep=False)]\n",
        "    print(f\"\\n중복된 image_id 목록 ({folder_path}):\")\n",
        "    print(duplicates[['image_id', 'filename']].sort_values('image_id').to_string(index=False))\n",
        "\n",
        "    # 중복된 데이터 제거 후 반환\n",
        "    df = df[df.duplicated(subset='image_id', keep=False) == False]\n",
        "    return df\n",
        "\n",
        "# 트레인과 검증 폴더 경로 설정\n",
        "train_folder = 'data/training_image'\n",
        "valid_folder = 'data/validation_image'\n",
        "\n",
        "# 트레인과 검증 폴더 처리\n",
        "train_df = process_folder(train_folder)\n",
        "valid_df = process_folder(valid_folder)\n",
        "\n",
        "# Gender와 Style별 이미지 개수 세기\n",
        "train_counts = train_df.groupby(['gender', 'style']).size().reset_index(name='count')\n",
        "valid_counts = valid_df.groupby(['gender', 'style']).size().reset_index(name='count')\n",
        "\n",
        "print(\"\\nTrain Image Counts by Gender and Style:\")\n",
        "print(train_counts)\n",
        "\n",
        "print(\"\\nValidation Image Counts by Gender and Style:\")\n",
        "print(valid_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-30T06:40:23.234931Z",
          "start_time": "2024-10-30T06:40:23.194704Z"
        },
        "id": "tlBGNtqSx8h3",
        "outputId": "b4c78c0f-c1e3-4ec5-c792-d680ab52a153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Class mapping:\n",
            "{('W', 'athleisure'): 0, ('W', 'bodyconscious'): 1, ('W', 'cityglam'): 2, ('W', 'classic'): 3, ('W', 'disco'): 4, ('W', 'ecology'): 5, ('W', 'feminine'): 6, ('W', 'genderless'): 7, ('W', 'grunge'): 8, ('W', 'hiphop'): 9, ('W', 'hippie'): 10, ('W', 'kitsch'): 11, ('W', 'lingerie'): 12, ('W', 'lounge'): 13, ('W', 'military'): 14, ('W', 'minimal'): 15, ('W', 'normcore'): 16, ('W', 'oriental'): 17, ('W', 'popart'): 18, ('W', 'powersuit'): 19, ('W', 'punk'): 20, ('W', 'space'): 21, ('W', 'sportivecasual'): 22, ('M', 'bold'): 23, ('M', 'hiphop'): 24, ('M', 'hippie'): 25, ('M', 'ivy'): 26, ('M', 'metrosexual'): 27, ('M', 'mods'): 28, ('M', 'normcore'): 29, ('M', 'sportivecasual'): 30}\n",
            "\n",
            "Class Mapping Table:\n",
            "    Class ID Gender           Style\n",
            "0          0      W      athleisure\n",
            "1          1      W   bodyconscious\n",
            "2          2      W        cityglam\n",
            "3          3      W         classic\n",
            "4          4      W           disco\n",
            "5          5      W         ecology\n",
            "6          6      W        feminine\n",
            "7          7      W      genderless\n",
            "8          8      W          grunge\n",
            "9          9      W          hiphop\n",
            "10        10      W          hippie\n",
            "11        11      W          kitsch\n",
            "12        12      W        lingerie\n",
            "13        13      W          lounge\n",
            "14        14      W        military\n",
            "15        15      W         minimal\n",
            "16        16      W        normcore\n",
            "17        17      W        oriental\n",
            "18        18      W          popart\n",
            "19        19      W       powersuit\n",
            "20        20      W            punk\n",
            "21        21      W           space\n",
            "22        22      W  sportivecasual\n",
            "23        23      M            bold\n",
            "24        24      M          hiphop\n",
            "25        25      M          hippie\n",
            "26        26      M             ivy\n",
            "27        27      M     metrosexual\n",
            "28        28      M            mods\n",
            "29        29      M        normcore\n",
            "30        30      M  sportivecasual\n"
          ]
        }
      ],
      "source": [
        "# 추출된 클래스 조합 확인 및 정렬\n",
        "class_combinations = train_df[['gender', 'style']].drop_duplicates()\n",
        "class_combinations = class_combinations.sort_values(['gender', 'style'], ascending=[False, True])\n",
        "\n",
        "# 정렬된 클래스 조합을 기반으로 클래스 매핑 생성\n",
        "class_mapping = {tuple(row): idx for idx, row in class_combinations.reset_index(drop=True).iterrows()}\n",
        "print(\"\\nClass mapping:\")\n",
        "print(class_mapping)\n",
        "\n",
        "# 클래스 ID 할당\n",
        "train_df['class_id'] = train_df.apply(lambda row: class_mapping[(row['gender'], row['style'])], axis=1)\n",
        "valid_df['class_id'] = valid_df.apply(lambda row: class_mapping[(row['gender'], row['style'])], axis=1)\n",
        "\n",
        "# 클래스 매핑 테이블 생성\n",
        "class_mapping_table = pd.DataFrame(list(class_mapping.keys()), columns=['Gender', 'Style'])\n",
        "class_mapping_table['Class ID'] = list(class_mapping.values())\n",
        "class_mapping_table = class_mapping_table[['Class ID', 'Gender', 'Style']]\n",
        "print(\"\\nClass Mapping Table:\")\n",
        "print(class_mapping_table)\n",
        "\n",
        "# CSV 파일로 저장\n",
        "train_df.to_csv('data/train_metadata_with_class.csv', index=False)\n",
        "valid_df.to_csv('data/valid_metadata_with_class.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-30T06:28:55.283734Z",
          "start_time": "2024-10-30T06:28:52.103670Z"
        },
        "id": "irTgRautx8h4",
        "outputId": "ddc76344-67e9-4fa7-dfa3-8231bae56bbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "GPU: Not available\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# GPU 사용 가능 여부 확인\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# 그래픽카드 정보 출력\n",
        "if torch.cuda.is_available():\n",
        "    print(\"\\nGPU:\")\n",
        "    print(torch.cuda.get_device_name(device))\n",
        "else:\n",
        "    print(\"\\nGPU: Not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "mission 1-2\n",
        "1. Focal Loss 구현\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, alpha=None, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        CE_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
        "        pt = torch.exp(-CE_loss)\n",
        "        F_loss = (1 - pt) ** self.gamma * CE_loss\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            alpha_factor = self.alpha.gather(0, targets)\n",
        "            F_loss *= alpha_factor\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return F_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return F_loss.sum()\n",
        "        else:\n",
        "            return F_loss\n",
        "설명:\n",
        "- Focal Loss는 클래스 불균형 문제를 완화하기 위한 손실 함수입니다. 잘못 분류된 샘플에 더 큰 가중치를 부여해 모델이 어려운 샘플에 더 집중하도록 합니다.\n",
        "- 파라미터: gamma는 손실의 형태를 조정하며, alpha는 특정 클래스에 가중치를 줄 때 사용합니다.\n",
        "\n",
        "Mission 1-2 요구 사항:\n",
        "Focal Loss는 불균형한 데이터셋에 대해 성능을 향상시킬 수 있습니다. 따라서 성능 향상을 위해 선택된 손실 함수입니다.\n",
        "\n",
        "2. ResNet-18 모델 아키텍처 구현\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        # 3x3 Conv 레이어 및 BatchNorm 설정\n",
        "        ...\n",
        "    def forward(self, x):\n",
        "        ...\n",
        "        return out\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ResNet18, self).__init__()\n",
        "        ...\n",
        "    def _make_layer(self, out_channels, num_blocks, stride):\n",
        "        ...\n",
        "    def forward(self, x):\n",
        "        ...\n",
        "        return out\n",
        "설명:\n",
        "- ResidualBlock: ResNet-18의 기본 단위로, 각 블록 내에는 두 개의 3x3 합성곱 레이어와 Batch Normalization 및 ReLU 활성화 함수가 사용됩니다. 입력과 출력을 더하는 **스킵 연결(shortcut connection)**이 포함되어 있어 기울기 소실 문제를 해결합니다.\n",
        "- ResNet-18: 입력 레이어, 여러 블록으로 이루어진 네 개의 레이어, 풀링 레이어 및 최종 분류 레이어로 구성됩니다.\n",
        "- num_classes 파라미터는 분류해야 하는 클래스 수입니다. 이 문제에서 클래스 수는 성별과 스타일 조합 수와 동일합니다.\n",
        "\n",
        "Mission 1-2 요구 사항:\n",
        "- ResNet-18 구조를 무작위 초기화된 가중치로 구현함으로써 전이 학습 없이 모델을 구성하고 학습할 수 있도록 했습니다.\n",
        "\n",
        "3. 데이터셋 클래스 정의 및 전처리\n",
        "class FashionDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        ...\n",
        "    def __len__(self):\n",
        "        ...\n",
        "    def __getitem__(self, idx):\n",
        "        ...\n",
        "        return image, label\n",
        "설명:\n",
        "- FashionDataset 클래스는 PyTorch의 Dataset을 상속받아 작성된 사용자 정의 데이터셋 클래스입니다.\n",
        "- __getitem__ 함수는 csv_file에서 filename을 통해 이미지를 불러오고, transform을 적용한 뒤 이미지와 클래스 ID(class_id)를 반환합니다.\n",
        "\n",
        "Mission 1-2 요구 사항:\n",
        "- 학습 데이터와 검증 데이터를 PyTorch에서 사용할 수 있도록 사용자 정의 데이터셋 클래스를 통해 구성합니다.\n",
        "\n",
        "4. Early Stopping 클래스\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, verbose=False, delta=0):\n",
        "        ...\n",
        "    def __call__(self, val_loss, model):\n",
        "        ...\n",
        "설명:\n",
        "- EarlyStopping은 검증 손실이 개선되지 않으면 학습을 멈추는 기능으로, 과적합(overfitting)을 방지할 수 있습니다. patience는 학습을 중단하기 전까지 허용되는 에폭 수입니다.\n",
        "\n",
        "Mission 1-2 요구 사항:\n",
        "- Early Stopping은 검증 손실을 기반으로 학습을 조기 종료하여 과적합을 방지하므로 성능 향상에 기여할 수 있습니다.\n",
        "\n",
        "5. 데이터 전처리 설정\n",
        "def get_transforms(augmentations=[]):\n",
        "    transform_list = [transforms.Resize((224, 224)), transforms.ToTensor(),\n",
        "                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
        "    ...\n",
        "    return transforms.Compose(transform_list)\n",
        "\n",
        "설명:\n",
        "get_transforms 함수는 기본 이미지 크기 조정 및 정규화를 수행하며, 학습 데이터에서는 수평 뒤집기 및 블러와 같은 데이터 증강을 추가하여 모델의 일반화 성능을 높입니다.\n",
        "\n",
        "6. 데이터셋 및 데이터로더 설정\n",
        "train_transform = get_transforms(augmentations=['horizontal_flip', 'gaussian_blur'])\n",
        "train_dataset = FashionDataset(csv_file='train_metadata_with_class.csv',\n",
        "                               root_dir=train_folder,\n",
        "                               transform=train_transform)\n",
        "\n",
        "valid_dataset = FashionDataset(csv_file='valid_metadata_with_class.csv',\n",
        "                               root_dir=valid_folder,\n",
        "                               transform=get_transforms())  # 검증 셋은 증강 없이\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
        "\n",
        "설명:\n",
        "학습용 데이터에서는 데이터 증강을 적용하여 train_loader에 데이터셋을 로드하며, 검증용 데이터는 증강 없이 valid_loader로 로드합니다.\n",
        "\n",
        "7. 모델, 옵티마이저, 스케줄러 및 손실 함수 설정\n",
        "model = ResNet18(num_classes=len(class_mapping)).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
        "criterion = FocalLoss(gamma=2)\n",
        "\n",
        "설명:\n",
        "AdamW 옵티마이저는 가중치 감쇠(weight decay)를 추가하여 과적합을 방지하고, 학습률 스케줄러 ReduceLROnPlateau는 검증 손실이 개선되지 않으면 학습률을 조정합니다.\n",
        "\n",
        "8. 학습 및 검증 루프\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    ...\n",
        "    \n",
        "    # 검증 루프\n",
        "    model.eval()\n",
        "    ...\n",
        "    \n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {accuracy:.2f}%')\n",
        "    \n",
        "    scheduler.step(val_loss)\n",
        "    early_stopping(val_loss, model)\n",
        "    \n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "설명:\n",
        "각 에폭에서 모델을 학습하고, 검증 손실과 정확도를 출력합니다.\n",
        "검증 후 scheduler.step과 early_stopping을 통해 학습률 조정 및 조기 종료 조건을 확인합니다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VvUcCynm8mne"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-10-30T03:53:58.928455Z",
          "start_time": "2024-10-30T03:53:55.761509Z"
        },
        "id": "NQquXpJJx8h4",
        "outputId": "06b32759-6d61-4be5-9c66-152137492211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "GPU: Not available\n"
          ]
        }
      ],
      "source": [
        "# Mission 1-2\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Focal Loss 구현\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, alpha=None, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        CE_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
        "        pt = torch.exp(-CE_loss)\n",
        "        F_loss = (1 - pt) ** self.gamma * CE_loss\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            alpha_factor = self.alpha.gather(0, targets)\n",
        "            F_loss *= alpha_factor\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return F_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return F_loss.sum()\n",
        "        else:\n",
        "            return F_loss\n",
        "\n",
        "# ResNet-18 아키텍처\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.shortcut(residual)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(ResidualBlock(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.maxpool(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# 데이터셋 클래스 정의\n",
        "class FashionDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.root_dir, row['filename'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = row['class_id']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Early Stopping 클래스 정의\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, verbose=False, delta=0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "\n",
        "# 전처리 적용 함수\n",
        "def get_transforms(augmentations=[]):\n",
        "    transform_list = [transforms.Resize((224, 224)), transforms.ToTensor(),\n",
        "                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
        "\n",
        "    # augmentations 목록에 따라 전처리 추가\n",
        "    if 'horizontal_flip' in augmentations:\n",
        "        transform_list.insert(0, transforms.RandomHorizontalFlip())\n",
        "    if 'gaussian_blur' in augmentations:\n",
        "        transform_list.insert(0, transforms.GaussianBlur(kernel_size=3))\n",
        "    if 'random_grayscale' in augmentations:\n",
        "        transform_list.insert(0, transforms.RandomGrayscale(p=0.2))\n",
        "\n",
        "    return transforms.Compose(transform_list)\n",
        "\n",
        "# 데이터셋 & 데이터로더 설정\n",
        "train_transform = get_transforms(augmentations=['horizontal_flip', 'gaussian_blur'])\n",
        "train_dataset = FashionDataset(csv_file='train_metadata_with_class.csv',\n",
        "                               root_dir=train_folder,\n",
        "                               transform=train_transform)\n",
        "\n",
        "valid_dataset = FashionDataset(csv_file='valid_metadata_with_class.csv',\n",
        "                               root_dir=valid_folder,\n",
        "                               transform=get_transforms())  # 검증 셋은 증강 없이\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
        "\n",
        "# 모델, 옵티마이저, 스케줄러, 손실 함수 설정\n",
        "model = ResNet18(num_classes=len(class_mapping)).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
        "criterion = FocalLoss(gamma=2)\n",
        "\n",
        "# Early Stopping 인스턴스\n",
        "early_stopping = EarlyStopping(patience=7)\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # 검증 루프\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(valid_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "    # 스케줄러와 Early Stopping 확인\n",
        "    scheduler.step(val_loss)\n",
        "    early_stopping(val_loss, model)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}